{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhirajBembade/AGC/blob/main/SGD2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WX480vztVfC-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sgd(\n",
        "    gradient, x, y, start, learn_rate=0.1, batch_size=1, n_iter=50,\n",
        "    tolerance=1e-06, dtype=\"float64\", random_state=None\n",
        "):\n",
        "    # Checking if the gradient is callable\n",
        "    if not callable(gradient):\n",
        "        raise TypeError(\"'gradient' must be callable\")\n",
        "\n",
        "    # Setting up the data type for NumPy arrays\n",
        "    dtype_ = np.dtype(dtype)\n",
        "\n",
        "    # Converting x and y to NumPy arrays\n",
        "    x, y = np.array(x, dtype=dtype_), np.array(y, dtype=dtype_)\n",
        "    n_obs = x.shape[0]\n",
        "    if n_obs != y.shape[0]:\n",
        "        raise ValueError(\"'x' and 'y' lengths do not match\")\n",
        "    xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n",
        "\n",
        "    # Initializing the random number generator\n",
        "    seed = None if random_state is None else int(random_state)\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "\n",
        "    # Initializing the values of the variables\n",
        "    vector = np.array(start, dtype=dtype_)\n",
        "\n",
        "    # Setting up and checking the learning rate\n",
        "    learn_rate = np.array(learn_rate, dtype=dtype_)\n",
        "    if np.any(learn_rate <= 0):\n",
        "        raise ValueError(\"'learn_rate' must be greater than zero\")\n",
        "\n",
        "    # Setting up and checking the size of minibatches\n",
        "    batch_size = int(batch_size)\n",
        "    if not 0 < batch_size <= n_obs:\n",
        "        raise ValueError(\n",
        "            \"'batch_size' must be greater than zero and less than \"\n",
        "            \"or equal to the number of observations\"\n",
        "        )\n",
        "\n",
        "    # Setting up and checking the maximal number of iterations\n",
        "    n_iter = int(n_iter)\n",
        "    if n_iter <= 0:\n",
        "        raise ValueError(\"'n_iter' must be greater than zero\")\n",
        "\n",
        "    # Setting up and checking the tolerance\n",
        "    tolerance = np.array(tolerance, dtype=dtype_)\n",
        "    if np.any(tolerance <= 0):\n",
        "        raise ValueError(\"'tolerance' must be greater than zero\")\n",
        "\n",
        "    # Performing the gradient descent loop\n",
        "    for _ in range(n_iter):\n",
        "        # Shuffle x and y\n",
        "        rng.shuffle(xy)\n",
        "\n",
        "        # Performing minibatch moves\n",
        "        for start in range(0, n_obs, batch_size):\n",
        "            stop = start + batch_size\n",
        "            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n",
        "\n",
        "            # Recalculating the difference\n",
        "            grad = np.array(gradient(x_batch, y_batch, vector), dtype_)\n",
        "            diff = -learn_rate * grad\n",
        "\n",
        "            # Checking if the absolute difference is small enough\n",
        "            if np.all(np.abs(diff) <= tolerance):\n",
        "                break\n",
        "\n",
        "            # Updating the values of the variables\n",
        "            vector += diff\n",
        "\n",
        "    return vector if vector.shape else vector.item()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ssr_gradient(x, y, b):\n",
        "    res = b[0] + b[1] * x - y\n",
        "    return res.mean(), (res * x).mean()  # .mean() is a method of np.ndarray\n",
        "x = np.array([5, 15, 25, 35, 45, 55])\n",
        "y = np.array([5, 20, 14, 32, 22, 38])"
      ],
      "metadata": {
        "id": "Rk3kcxxrWb7t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With batch_size, you specify the number of observations in each minibatch. This is an essential parameter for stochastic gradient descent that can significantly affect performance. Lines 34 to 39 ensure that batch_size is a positive integer no larger than the total number of observations.\n",
        "\n",
        "Another new parameter is random_state. It defines the seed of the random number generator on line 22. The seed is used on line 23 as an argument to default_rng(), which creates an instance of Generator.\n",
        "\n",
        "If you pass the argument None for random_state, then the random number generator will return different numbers each time itâ€™s instantiated. If you want each instance of the generator to behave exactly the same way, then you need to specify seed. The easiest way is to provide an arbitrary integer.\n",
        "\n",
        "Line 16 deduces the number of observations with x.shape[0]. If x is a one-dimensional array, then this is its size. If x has two dimensions, then .shape[0] is the number of rows.\n",
        "\n",
        "On line 19, you use .reshape() to make sure that both x and y become two-dimensional arrays with n_obs rows and that y has exactly one column. numpy.c_[] conveniently concatenates the columns of x and y into a single array, xy. This is one way to make data suitable for random selection.\n",
        "\n",
        "Finally, on lines 52 to 70, you implement the for loop for the stochastic gradient descent. It differs from gradient_descent(). On line 54, you use the random number generator and its method .shuffle() to shuffle the observations. This is one of the ways to choose minibatches randomly.\n",
        "\n",
        "The inner for loop is repeated for each minibatch. The main difference from the ordinary gradient descent is that, on line 62, the gradient is calculated for the observations from a minibatch (x_batch and y_batch) instead of for all observations (x and y).\n",
        "\n",
        "On line 59, x_batch becomes a part of xy that contains the rows of the current minibatch (from start to stop) and the columns that correspond to x. y_batch holds the same rows from xy but only the last column (the outputs)."
      ],
      "metadata": {
        "id": "zIgKSFIzV_5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sgd( ssr_gradient, x, y, start=[0.5, 0.5], learn_rate=0.0008,batch_size=3, n_iter=100_000, random_state=0 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7icwKJoVV1Bt",
        "outputId": "93e32ebb-33a5-4ee6-8d85-080c5aac8e10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.63093736, 0.53982921])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result is an array with two values that correspond to the decision variables: ð‘â‚€ = 5.63 and ð‘â‚ = 0.54. The best regression line is ð‘“(ð‘¥) = 5.63 + 0.54ð‘¥. As in the previous examples, this result heavily depends on the learning rate."
      ],
      "metadata": {
        "id": "fcbuwM_aW1ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wUp7ZrEcW2f8"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}